{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Content</h1>\n",
    "<ul>\n",
    "    <li><a href='#torch_start'>Torch Start</a></li>\n",
    "    <ul>\n",
    "        <li><a href='#tensors'>Tensors</a></li>\n",
    "        <li><a href='#basic_operations'>Basic Operations</a></li>        \n",
    "        <li><a href='#numpy_and_tensor'>Numpy And Tensor</a></li>        \n",
    "        <li><a href='#indexing'>Indexing</a></li>\n",
    "        <li><a href='#reshape'>Reshape</a></li>\n",
    "        <li><a href='#tensor_on_GPU'>Tensor on GPU</a></li>\n",
    "        <li><a href='#auto_gradient'>auto gradient</a></li>\n",
    "        <li><a href='#deatach'>Deatach</a></li>        \n",
    "    </ul>\n",
    "    <li><a href='#linear_regression'>Linear Regression</a></li>        \n",
    "    <li><a href='#logistic_regression'>Logistic Regression</a></li>        \n",
    "    <li><a href='#standard_scaler'>StandardScaler</a></li>        \n",
    "    <li><a href='#iris_classification'>Iris Classification</a></li>    \n",
    "    <li><a href='#custom_dataset'>Custom Dataset</a></li>\n",
    "    <li><a href='#split_dataset'>Split Dataset</a></li>\n",
    "    <li><a href='#transform'>Transform</a></li>\n",
    "    <li><a href='#custom_transform'>Custom transform</a></li>    \n",
    "    <li><a href='#softmax_and_crossentropy'>Softmax and Crossentropy</a></li>    \n",
    "    <li><a href='#MNIST_classification_feed_forward'>MNIST Classification Feed Forward</a></li>    \n",
    "    <li><a href='#conv_net_with_CIFAR10'>Convolution Network with CIFAR10</a></li>    \n",
    "    <li><a href='#transfer_learning'>Transfer Learning</a></li> \n",
    "    <ul><li>Working with Images Folder</ul>\n",
    "    <li><a href='#model_save_and_load'>Model save and load</a></li>\n",
    "    <li><a href='#save_checkpoints'>Save & Load Checkpoints</a></li>\n",
    "    <li><a href='#tensorboard'>Tensorboard</a></li>\n",
    "    <li><a href='#links'>Links</a></li>\n",
    "</ul>\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='torch_start' name='torch_start'><h1>Torch Start </h1></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-09-22T11:22:00.977255Z",
     "iopub.status.busy": "2021-09-22T11:22:00.976916Z",
     "iopub.status.idle": "2021-09-22T11:22:02.096324Z",
     "shell.execute_reply": "2021-09-22T11:22:02.095611Z",
     "shell.execute_reply.started": "2021-09-22T11:22:00.977226Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='tensors' name='tensors' style='padding:0'><h2>Tensors</h2><pre>\n",
    "<ul><li>Everything in pytorch is based on Tensor operations.\n",
    "<li>A tensor can have different dimensions, so it can be 1d, 2d, or even 3d and higher\n",
    "<li>scalar, vector, matrix, tensor</ul></pre>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-10T13:38:43.832845Z",
     "iopub.status.busy": "2021-09-10T13:38:43.832231Z",
     "iopub.status.idle": "2021-09-10T13:38:43.955149Z",
     "shell.execute_reply": "2021-09-10T13:38:43.953879Z",
     "shell.execute_reply.started": "2021-09-10T13:38:43.832808Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_scalar:  tensor([-2.2299e-29])\n",
      "x_vector:  tensor([-4.5627e+19,  4.5562e-41, -4.5627e+19])\n",
      "x_2d:  tensor([[-4.5627e+19,  4.5562e-41, -4.5627e+19],\n",
      "        [ 4.5562e-41,  4.4842e-44,  0.0000e+00]])\n",
      "x_3d:  tensor([[[-4.5627e+19,  4.5562e-41, -2.2270e-29],\n",
      "         [ 3.0858e-41,  1.4013e-45,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n"
     ]
    }
   ],
   "source": [
    "# torch from list\n",
    "tensor=torch.tensor([[1,2,3],[4,5,6]])\n",
    "# torch.empty(size): uninitiallized\n",
    "x_scalar = torch.empty(1) # scalar\n",
    "print('x_scalar: ',x_scalar)\n",
    "x_vector = torch.empty(3) # vector, 1D\n",
    "print('x_vector: ',x_vector)\n",
    "x_2d = torch.empty(2,3) # matrix, 2D\n",
    "print('x_2d: ',x_2d)\n",
    "x_3d = torch.empty(2,2,3) # tensor, 3 dimensions\n",
    "#x = torch.empty(2,2,2,3) # tensor, 4 dimensions\n",
    "print('x_3d: ',x_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-21T13:43:13.262347Z",
     "iopub.status.busy": "2021-09-21T13:43:13.261965Z",
     "iopub.status.idle": "2021-09-21T13:43:13.276072Z",
     "shell.execute_reply": "2021-09-21T13:43:13.274726Z",
     "shell.execute_reply.started": "2021-09-21T13:43:13.262313Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ones:  tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "random_nums:  tensor([[0.2527, 0.6562, 0.4066, 0.3594],\n",
      "        [0.7350, 0.9694, 0.1607, 0.1710],\n",
      "        [0.6109, 0.1270, 0.0640, 0.1335],\n",
      "        [0.2526, 0.3895, 0.2576, 0.8987]])\n",
      "eyes:  tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n",
      "random_nums[1,1]:  tensor(0.9694)\n",
      "scalar tensor value:  0.9694017767906189\n"
     ]
    }
   ],
   "source": [
    "# torch.zeros(size), fill with 0\n",
    "# torch.ones(size), fill with 1\n",
    "# torch.rand(size), fill with random numbers\n",
    "\n",
    "ones=torch.ones(4,4)\n",
    "random_nums=torch.rand(4,4)\n",
    "eyes=torch.eye(4,4)\n",
    "print('ones: ',ones)\n",
    "print('random_nums: ',random_nums)\n",
    "print('eyes: ',eyes)\n",
    "# Get the actual value if only 1 element in your tensor\n",
    "print('random_nums[1,1]: ',random_nums[1,1])\n",
    "print('scalar tensor value: ',random_nums[1,1].item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range of numbers from 0 to 200 with step 10 tensor([  0,  10,  20,  30,  40,  50,  60,  70,  80,  90, 100, 110, 120, 130,\n",
      "        140, 150, 160, 170, 180, 190])\n"
     ]
    }
   ],
   "source": [
    "# range of numbers\n",
    "range_nums=torch.arange(start=0, end=200,step=10)\n",
    "print('range of numbers from 0 to 200 with step 10', range_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size torch.Size([3, 4])\n",
      "shape torch.Size([3, 4])\n",
      "torch.float32\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "# check size\n",
    "x=torch.rand(3,4)\n",
    "print('size',x.size())\n",
    "print('shape',x.shape)\n",
    "# check data type\n",
    "print(x.dtype)\n",
    "\n",
    "# specify types, float32 default\n",
    "x = torch.zeros(5, 3, dtype=torch.float16)\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specify tensor device\n",
    "tensor_with_device=torch.tensor([[1,2,3],[4,5,6]], dtype=torch.float16, device='cpu')\n",
    "tensor_with_device.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='basic_operations' name='basic_operations'><h2>Basic Operations</h2></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-21T13:45:54.424973Z",
     "iopub.status.busy": "2021-09-21T13:45:54.424333Z",
     "iopub.status.idle": "2021-09-21T13:45:54.437653Z",
     "shell.execute_reply": "2021-09-21T13:45:54.436446Z",
     "shell.execute_reply.started": "2021-09-21T13:45:54.424934Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "x=torch.rand(2,3)\n",
    "y=torch.rand(2,3)\n",
    "#add\n",
    "z=x.add(y)\n",
    "z=torch.add(x,y)\n",
    "z=x+y\n",
    "#subtract\n",
    "z=x.sub(y)\n",
    "z=torch.sub(x,y)\n",
    "z=x-y\n",
    "#multiplication\n",
    "z=x.mul(y)\n",
    "z=x*y\n",
    "#division\n",
    "z=x.div(y)\n",
    "z=torch.divide(x,y)\n",
    "z=torch.true_divide(x,y)\n",
    "z=x/y\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='numpy_and_tensor' name='numpy_and_tensor'><h2>Numpy and Tensor </h2></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# from tensor to numpy array\n",
    "x=torch.Tensor(3,4)\n",
    "xnp=x.numpy()\n",
    "x,xnp\n",
    "# from numpy array to tensor\n",
    "xnp=np.random.rand(5,6)\n",
    "x=torch.from_numpy(xnp)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='indexing' name='indexing'><h2>Indexing </h2></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-22T11:22:56.97179Z",
     "iopub.status.busy": "2021-09-22T11:22:56.97142Z",
     "iopub.status.idle": "2021-09-22T11:22:57.048402Z",
     "shell.execute_reply": "2021-09-22T11:22:57.047511Z",
     "shell.execute_reply.started": "2021-09-22T11:22:56.971759Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "x=torch.rand((3,5))\n",
    "print(x[0,:3])\n",
    "rows=torch.tensor([1,0])\n",
    "cols=torch.tensor([4,0])\n",
    "\n",
    "print(x)\n",
    "# specific rows and columns with order\n",
    "print(x[rows,cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='reshape' name='reshape'><h2>Reshape </h2></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-01T12:21:40.968136Z",
     "iopub.status.busy": "2021-09-01T12:21:40.967793Z",
     "iopub.status.idle": "2021-09-01T12:21:40.977499Z",
     "shell.execute_reply": "2021-09-01T12:21:40.976049Z",
     "shell.execute_reply.started": "2021-09-01T12:21:40.968105Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# reshape\n",
    "x=torch.rand(3,4)\n",
    "y=x.view(12)\n",
    "print(x.shape,y.shape)\n",
    "z=x.view(-1)\n",
    "print(z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='tensor_on_GPU' name='tensor_on_GPU'><h2>Tensor on GPU </h2></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-01T12:24:06.46339Z",
     "iopub.status.busy": "2021-09-01T12:24:06.463025Z",
     "iopub.status.idle": "2021-09-01T12:24:06.469788Z",
     "shell.execute_reply": "2021-09-01T12:24:06.468587Z",
     "shell.execute_reply.started": "2021-09-01T12:24:06.463359Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# by default all tensors are created on the CPU,\n",
    "# but you can also move them to the GPU (only if it's available )\n",
    "# tensor\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "x=torch.tensor([2,3,4,5],dtype=torch.float32, device=device,requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n",
    "    x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n",
    "    z = x + y\n",
    "    # z = z.numpy() # not possible because numpy cannot handle GPU tenors\n",
    "    # move to CPU again\n",
    "    z.to(\"cpu\")       # ``.to`` can also change dtype together!\n",
    "    # z = z.numpy()\n",
    "    print(x)\n",
    "    print(y)\n",
    "    print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='auto_gradient' name='auto_gradient'><h2>Auto Gradient </h2></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# tensor need to calculate gradient later in optimization\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "y = x + 2\n",
    "\n",
    "# y was created as a result of an operation, so it has a grad_fn attribute.\n",
    "# grad_fn: references a Function that has created the Tensor\n",
    "print(x) # created by the user -> grad_fn is None\n",
    "print(y)\n",
    "print(y.grad_fn)\n",
    "\n",
    "# Do more operations on y\n",
    "z = y * y * 3\n",
    "print(z)\n",
    "z = z.mean()\n",
    "print(z)\n",
    "\n",
    "z.backward()\n",
    "print(x.grad) # dz/dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='deatach' name='deatach'><h2>Deatach </h2></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-02T07:08:55.538513Z",
     "iopub.status.busy": "2021-09-02T07:08:55.538029Z",
     "iopub.status.idle": "2021-09-02T07:08:55.545118Z",
     "shell.execute_reply": "2021-09-02T07:08:55.544151Z",
     "shell.execute_reply.started": "2021-09-02T07:08:55.538482Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# .detach(): get a new Tensor with the same content but no gradient computation:\n",
    "a = torch.randn(2, 2, requires_grad=True)\n",
    "print(a.requires_grad)\n",
    "b = a.detach()\n",
    "print(b.requires_grad)\n",
    "\n",
    "#or using\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='linear_regression' name='linear_regression'><h1>Linear Regression </h1></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-02T07:33:11.722083Z",
     "iopub.status.busy": "2021-09-02T07:33:11.72139Z",
     "iopub.status.idle": "2021-09-02T07:33:11.729167Z",
     "shell.execute_reply": "2021-09-02T07:33:11.727348Z",
     "shell.execute_reply.started": "2021-09-02T07:33:11.722038Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# f=w*x\n",
    "x=np.array([1,2,3,4],dtype=np.float32)\n",
    "y=np.array([3,6,9,12],dtype=np.float32)\n",
    "\n",
    "def gradient(x, y, y_pred):\n",
    "    return np.dot(2*x, y_pred - y).mean()\n",
    "\n",
    "gradient(2,6,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-22T12:51:58.773039Z",
     "iopub.status.busy": "2021-09-22T12:51:58.772610Z",
     "iopub.status.idle": "2021-09-22T12:52:00.966859Z",
     "shell.execute_reply": "2021-09-22T12:52:00.965959Z",
     "shell.execute_reply.started": "2021-09-22T12:51:58.772951Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-22T12:52:05.966618Z",
     "iopub.status.busy": "2021-09-22T12:52:05.966277Z",
     "iopub.status.idle": "2021-09-22T12:52:06.000137Z",
     "shell.execute_reply": "2021-09-22T12:52:05.999072Z",
     "shell.execute_reply.started": "2021-09-22T12:52:05.966589Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 0) Prepare data\n",
    "X_numpy, y_numpy = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=4)\n",
    "\n",
    "# cast to float Tensor\n",
    "X = torch.from_numpy(X_numpy.astype(np.float32))\n",
    "y = torch.from_numpy(y_numpy.astype(np.float32))\n",
    "y = y.view(y.shape[0], 1)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "# 1) Model\n",
    "# Linear model f = wx + b\n",
    "input_size = n_features\n",
    "output_size = 1\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "# 2) Loss and optimizer\n",
    "learning_rate = 0.01\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-22T12:52:28.343003Z",
     "iopub.status.busy": "2021-09-22T12:52:28.342646Z",
     "iopub.status.idle": "2021-09-22T12:52:28.777110Z",
     "shell.execute_reply": "2021-09-22T12:52:28.776384Z",
     "shell.execute_reply.started": "2021-09-22T12:52:28.342973Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 3) Training loop\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass and loss\n",
    "    y_predicted = model(X)\n",
    "    loss = criterion(y_predicted, y)\n",
    "    \n",
    "    # Backward pass and update\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # zero grad before new step\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
    "\n",
    "# Plot\n",
    "predicted = model(X).detach().numpy()\n",
    "\n",
    "plt.plot(X_numpy, y_numpy, 'ro')\n",
    "plt.plot(X_numpy, predicted, 'b')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-22T12:53:19.128485Z",
     "iopub.status.busy": "2021-09-22T12:53:19.127883Z",
     "iopub.status.idle": "2021-09-22T12:53:19.149364Z",
     "shell.execute_reply": "2021-09-22T12:53:19.148334Z",
     "shell.execute_reply.started": "2021-09-22T12:53:19.128450Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "def accuracy_fun(model, X,Y, pct):\n",
    "    n = len(X)\n",
    "    with torch.no_grad():\n",
    "        oupt = model(X)      # all computed prices\n",
    "    max_deltas = torch.abs(pct * Y)    # max allowable deltas\n",
    "    abs_deltas = torch.abs(oupt - Y)   # actual differences\n",
    "    results = abs_deltas < max_deltas  # [[True, False, . .]]\n",
    "    acc = torch.sum(results, dim=0).item() / n  # dim not needed\n",
    "    return acc\n",
    "\n",
    "acc = accuracy_fun(model, X,y, 0.10)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='logistic regression' name='logistic regression'><h1>logistic Regression</h1></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-22T13:21:17.000542Z",
     "iopub.status.busy": "2021-09-22T13:21:17.000124Z",
     "iopub.status.idle": "2021-09-22T13:21:17.056003Z",
     "shell.execute_reply": "2021-09-22T13:21:17.054984Z",
     "shell.execute_reply.started": "2021-09-22T13:21:17.000503Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div id='standard_scaler' name='standard_scaler'><h2>StandardScaler</h2><pre>\n",
    "<ul><li>To center the data (make it have zero mean and unit standard error), you subtract the mean and then divide the result by the standard deviation: x′=x−μ/σ\n",
    "<li>You do that on the training set of the data. But then you have to apply the same transformation to your test set (e.g. in cross-validation), or to newly obtained examples before forecasting. \n",
    "<li>But you have to use the exact same two parameters μ and σ (values) that you used for centering the training set.\n",
    "<li>Hence, every scikit-learn's transform's fit() just calculates the parameters (e.g. μ and σ in case of StandardScaler) and saves them as an internal object's state. \n",
    "<li>Afterwards, you can call its transform() method to apply the transformation to any particular set of examples.\n",
    "<li>fit_transform() joins these two steps and is used for the initial fitting of parameters on the training set x, while also returning the transformed x′. Internally, the transformer object just calls first fit() and then transform() on the same data.</ul></pre>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-22T13:21:19.285752Z",
     "iopub.status.busy": "2021-09-22T13:21:19.285391Z",
     "iopub.status.idle": "2021-09-22T13:21:19.318190Z",
     "shell.execute_reply": "2021-09-22T13:21:19.317064Z",
     "shell.execute_reply.started": "2021-09-22T13:21:19.285717Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 0) Prepare data\n",
    "bc = datasets.load_breast_cancer()\n",
    "X, y = bc.data, bc.target\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
    "\n",
    "# scale\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
    "\n",
    "y_train = y_train.view(y_train.shape[0], 1)\n",
    "y_test = y_test.view(y_test.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-22T13:21:20.171016Z",
     "iopub.status.busy": "2021-09-22T13:21:20.170508Z",
     "iopub.status.idle": "2021-09-22T13:21:20.178913Z",
     "shell.execute_reply": "2021-09-22T13:21:20.177711Z",
     "shell.execute_reply.started": "2021-09-22T13:21:20.170974Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 1) Model\n",
    "# Linear model f = wx + b , sigmoid at the end\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, n_input_features):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = nn.Linear(n_input_features, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_pred = torch.sigmoid(self.linear(x))\n",
    "        return y_pred\n",
    "\n",
    "model = Model(n_features)\n",
    "\n",
    "# 2) Loss and optimizer\n",
    "num_epochs = 100\n",
    "learning_rate = 0.01\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-22T13:21:21.149329Z",
     "iopub.status.busy": "2021-09-22T13:21:21.148987Z",
     "iopub.status.idle": "2021-09-22T13:21:21.207678Z",
     "shell.execute_reply": "2021-09-22T13:21:21.206455Z",
     "shell.execute_reply.started": "2021-09-22T13:21:21.149297Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 3) Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass and loss\n",
    "    y_pred = model(X_train)\n",
    "    loss = criterion(y_pred, y_train)\n",
    "\n",
    "    # Backward pass and update\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # zero grad before new step\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-22T13:21:29.131584Z",
     "iopub.status.busy": "2021-09-22T13:21:29.131244Z",
     "iopub.status.idle": "2021-09-22T13:21:29.142475Z",
     "shell.execute_reply": "2021-09-22T13:21:29.141430Z",
     "shell.execute_reply.started": "2021-09-22T13:21:29.131555Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_predicted = model(X_test)\n",
    "    y_predicted_cls = y_predicted.round()\n",
    "    acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
    "    print(f'accuracy: {acc.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='iris_classification' name='iris_classification'><h1>Iris Classification</h1></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-22T13:31:11.635197Z",
     "iopub.status.busy": "2021-09-22T13:31:11.634778Z",
     "iopub.status.idle": "2021-09-22T13:31:11.642214Z",
     "shell.execute_reply": "2021-09-22T13:31:11.641055Z",
     "shell.execute_reply.started": "2021-09-22T13:31:11.635166Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-22T13:32:03.440336Z",
     "iopub.status.busy": "2021-09-22T13:32:03.439983Z",
     "iopub.status.idle": "2021-09-22T13:32:03.887758Z",
     "shell.execute_reply": "2021-09-22T13:32:03.886317Z",
     "shell.execute_reply.started": "2021-09-22T13:32:03.440307Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "# Read the IRIS dataset\n",
    "df = pd.read_csv(\n",
    "    \"https://data.heatonresearch.com/data/t81-558/iris.csv\", \n",
    "    na_values=['NA', '?'])\n",
    "\n",
    "# Convert to numpy - Classification\n",
    "x = df[['sepal_l', 'sepal_w', 'petal_l', 'petal_w']].values\n",
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(df['species'])\n",
    "species = le.classes_\n",
    "\n",
    "# Split into validation and training sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(    \n",
    "    x, y, test_size=0.25, random_state=42)\n",
    "\n",
    "x_train = Variable(torch.Tensor(x_train).float())\n",
    "x_test = Variable(torch.Tensor(x_test).float())\n",
    "y_train = Variable(torch.LongTensor(y_train))\n",
    "y_test = Variable(torch.LongTensor(y_test))\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-22T13:32:06.471414Z",
     "iopub.status.busy": "2021-09-22T13:32:06.471057Z",
     "iopub.status.idle": "2021-09-22T13:32:06.479041Z",
     "shell.execute_reply": "2021-09-22T13:32:06.477577Z",
     "shell.execute_reply.started": "2021-09-22T13:32:06.471381Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, in_count, out_count):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_count, 50)\n",
    "        self.fc2 = nn.Linear(50, 25)\n",
    "        self.fc3 = nn.Linear(25, out_count)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return self.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-22T13:32:08.445330Z",
     "iopub.status.busy": "2021-09-22T13:32:08.444992Z",
     "iopub.status.idle": "2021-09-22T13:32:08.825298Z",
     "shell.execute_reply": "2021-09-22T13:32:08.824002Z",
     "shell.execute_reply.started": "2021-09-22T13:32:08.445301Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model = Net(x.shape[1],len(species))\n",
    "criterion = nn.CrossEntropyLoss()# cross entropy loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(x_train)\n",
    "    loss = criterion(out, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch+1)%20 == 0:\n",
    "        print(f\"Epoch {epoch+1}, loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-22T13:32:17.913937Z",
     "iopub.status.busy": "2021-09-22T13:32:17.913575Z",
     "iopub.status.idle": "2021-09-22T13:32:17.923153Z",
     "shell.execute_reply": "2021-09-22T13:32:17.922444Z",
     "shell.execute_reply.started": "2021-09-22T13:32:17.913909Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "with torch.no_grad():\n",
    "    pred = model(x_test)\n",
    "    _, predict_classes = torch.max(pred, 1)\n",
    "\n",
    "    correct = accuracy_score(y_test,predict_classes)\n",
    "    print(f\"Accuracy: {correct}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-09-22T13:32:31.727900Z",
     "iopub.status.busy": "2021-09-22T13:32:31.727411Z",
     "iopub.status.idle": "2021-09-22T13:32:31.732845Z",
     "shell.execute_reply": "2021-09-22T13:32:31.731859Z",
     "shell.execute_reply.started": "2021-09-22T13:32:31.727867Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "torch.max?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='custom_dataset' name='custom_dataset'><h1>Custom Dataset</h1></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn # All neural network modules, nn.Linear, nn.Conv2d, BatchNorm, Loss functions\n",
    "import torch.optim as optim # For all Optimization algorithms, SGD, Adam, etc.\n",
    "import torch.nn.functional as F # All functions that don't have any parameters\n",
    "from torch.utils.data import DataLoader # Gives easier dataset managment and creates mini batches\n",
    "import torchvision.datasets as datasets # Has standard datasets we can import in a nice and easy way\n",
    "import torchvision.transforms as transforms # Transformations we can perform on our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define torch Dataset \n",
    "* create class inheret Dataset class\n",
    ">* implemet __init__ function\n",
    ">* implemet __getitem__ function\n",
    ">* implemet __len__ function\n",
    "* instantiate object of the dataset class\n",
    "* dataloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "config={\n",
    "    'batch_size' : 4,\n",
    "    'num_workers' : 2,\n",
    "    'epochs' : 100\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-22T11:13:57.045763Z",
     "iopub.status.busy": "2021-09-22T11:13:57.045296Z",
     "iopub.status.idle": "2021-09-22T11:13:57.120778Z",
     "shell.execute_reply": "2021-09-22T11:13:57.11932Z",
     "shell.execute_reply.started": "2021-09-22T11:13:57.045645Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class WineDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        data=np.load_txt('wine.csv')\n",
    "        #wine_data = np.loadtxt('pytorchTutorial-master/data/wine/wine.csv', delimiter=',', dtype=np.float32, skiprows=1)\n",
    "        self.x_train = torch.from_numpy(wine_data[:, 1:])\n",
    "        self.y_train = torch.from_numpy(wine_data[:, 0])\n",
    "        \n",
    "        self.n_samples=wine_data.shape[0]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_train[index], self.y_train[index]\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dataset=WineDataset()\n",
    "train_loader=DataLoader(\n",
    "    dataset = dataset, \n",
    "    batch_size=config['batch_size'], \n",
    "    num_workers=config['num_workers'], \n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "n_iterations=dataset.__len__()/config['batch_size']\n",
    "for epoch in range(config['epochs']):\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        pass\n",
    "    if (epoch+1)% 10 ==0:\n",
    "        print(f'Epoch: {epoch+1}/{config[\"epochs\"]}, Step {i+1}/{n_iterations}| Inputs {inputs.shape} | Labels {labels.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='split_dataset' name='split_dataset'><h1>Split Dataset</h1></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn  # All neural network modules, nn.Linear, nn.Conv2d, BatchNorm, Loss functions\n",
    "import torch.optim as optim  # For all Optimization algorithms, SGD, Adam, etc.\n",
    "import torchvision.transforms as transforms  # Transformations we can perform on our dataset\n",
    "import torchvision\n",
    "import os\n",
    "import pandas as pd\n",
    "from skimage import io\n",
    "from torch.utils.data import (\n",
    "    Dataset,\n",
    "    DataLoader,\n",
    ")  # Gives easier dataset managment and creates mini batches\n",
    "\n",
    "\n",
    "class CatsAndDogsDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.root_dir, self.annotations.iloc[index, 0])\n",
    "        image = io.imread(img_path)\n",
    "        y_label = torch.tensor(int(self.annotations.iloc[index, 1]))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return (image, y_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "in_channel = 3\n",
    "num_classes = 2\n",
    "learning_rate = 1e-3\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "\n",
    "# Load Data\n",
    "dataset = CatsAndDogsDataset(\n",
    "    csv_file=\"cats_dogs.csv\",\n",
    "    root_dir=\"cats_dogs_resized\",\n",
    "    transform=transforms.ToTensor(),\n",
    ")\n",
    "\n",
    "# Dataset is actually a lot larger ~25k images, just took out 10 pictures\n",
    "# to upload to Github. It's enough to understand the structure and scale\n",
    "# if you got more images.\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [5, 5])\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='transform' name='transform'><h1>Transform</h1></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "#import torch.nn as nns\n",
    "#import torch.nn.functional as F\n",
    "#import torchvision\n",
    "#import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='custom_transform' name='custom_transform'><h1>Custom transform</h1></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class WineDataset(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        wine_data = np.loadtxt('pytorchTutorial-master/data/wine/wine.csv', delimiter=',', dtype=np.float32, skiprows=1)\n",
    "        x_train = wine_data[:, 1:]\n",
    "        y_train = wine_data[: 0]\n",
    "        \n",
    "        self.n_samples = wine_data.shape[0]\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        sample = self.x_data[index], self.y_data[index]\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "class ToTensor:\n",
    "    def __call__(self, sample):\n",
    "        inputs, targets = sample\n",
    "        return torch.from_numpy(inputs), torch.from_numpy(targets)\n",
    "    \n",
    "class MulTransform:\n",
    "    # multiply inputs with a given factor\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        inputs, targets = sample\n",
    "        inputs *= self.factor\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dataset=WineDataset(transform=ToTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print('\\nWith Tensor and Multiplication Transform')\n",
    "composed = torchvision.transforms.Compose([ToTensor(), MulTransform(4)])\n",
    "dataset = WineDataset(transform=composed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='softmax_and_crossentropy' name='softmax_and_crossentropy'><h1>Softmax and crossentropy</h1>\n",
    "<h2>Softmax</h2>\n",
    "<ul><li>Softmax applies the exponential function to each element, and normalizes by dividing by the sum of all these exponentials\n",
    "<li>squashes the output to be between 0 and 1 = probability\n",
    "<li>sum of all probabilities is 1</ul>\n",
    "    <h2>Cross entropy</h2>\n",
    "<ul><li>Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. \n",
    "<li>loss increases as the predicted probability diverges from the actual label\n",
    "    </ul>\n",
    "    <h2>CrossEntropyLoss in PyTorch (applies Softmax)</h2>\n",
    "<ul><li>nn.LogSoftmax + nn.NLLLoss\n",
    "<li>NLLLoss = negative log likelihood loss\n",
    "    </ul>\n",
    "<pre>loss = nn.CrossEntropyLoss() \n",
    "loss(input, target)\n",
    "target is of size nSamples = 1\n",
    "each element has class label: 0, 1, or 2\n",
    "Y (=target) contains class labels, not one-hot</pre>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# Binary classification\n",
    "class NeuralNet1(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNet1, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, 1)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        # sigmoid at the end\n",
    "        y_pred = torch.sigmoid(out)\n",
    "        return y_pred\n",
    "\n",
    "model = NeuralNet1(input_size=28*28, hidden_size=5)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Multiclass problem\n",
    "class NeuralNet2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet2, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        # no softmax at the end\n",
    "        return out\n",
    "\n",
    "model = NeuralNet2(input_size=28*28, hidden_size=5, num_classes=3)\n",
    "criterion = nn.CrossEntropyLoss()  # (applies Softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='MNIST_classification_feed_forward' name='MNIST_classification_feed_forward'><h1>MNIST Classification Feed Forward</h1></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters \n",
    "input_size = 784 # 28x28\n",
    "hidden_size = 500 \n",
    "num_classes = 10\n",
    "num_epochs = 2\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# MNIST dataset \n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),  \n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "examples = iter(test_loader)\n",
    "example_data, example_targets = examples.next()\n",
    "\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.imshow(example_data[i][0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Fully connected neural network with one hidden layer\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.l1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        # no activation and no softmax at the end\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "# Train the model\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # origin shape: [100, 1, 28, 28]\n",
    "        # resized: [100, 784]\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        # max returns (value ,index)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network on the 10000 test images: {acc} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='conv_net_with_CIFAR10' name='conv_net_with_CIFAR10'><h1>Convolution Network with CIFAR10</h1></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters \n",
    "num_epochs = 5\n",
    "batch_size = 4\n",
    "learning_rate = 0.001\n",
    "\n",
    "# dataset has PILImage images of range [0, 1]. \n",
    "# We transform them to Tensors of normalized range [-1, 1]\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# CIFAR10: 60000 32x32 color images in 10 classes, with 6000 images per class\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                         shuffle=False)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "transforms.Normalize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # -> n, 3, 32, 32\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # -> n, 6, 14, 14\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # -> n, 16, 5, 5\n",
    "        x = x.view(-1, 16 * 5 * 5)            # -> n, 400\n",
    "        x = F.relu(self.fc1(x))               # -> n, 120\n",
    "        x = F.relu(self.fc2(x))               # -> n, 84\n",
    "        x = self.fc3(x)                       # -> n, 10\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "images,labels=next(iter(train_loader))\n",
    "conv1=nn.Conv2d(3,6,5)\n",
    "pool1=nn.MaxPool2d(2,2)\n",
    "conv2=nn.Conv2d(6,16,5)\n",
    "pool2=nn.MaxPool2d(2,2)\n",
    "\n",
    "conv1_out=conv1((torch.unsqueeze(images[0], dim=0)))\n",
    "pool1_out=pool((conv1_out))\n",
    "conv2_out=conv2((pool1_out))\n",
    "pool2_out=pool((conv2_out))\n",
    "\n",
    "#imshow(conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pool2_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "nn.Conv2d?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model = ConvNet().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # origin shape: [4, 3, 32, 32] = 4, 3, 1024\n",
    "        # input_layer: 3 input channels, 6 output channels, 5 kernel size\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 2000 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print('Finished Training')\n",
    "PATH = './cnn.pth'\n",
    "torch.save(model.state_dict(), PATH)\n",
    "\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(10)]\n",
    "    n_class_samples = [0 for i in range(10)]\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        # max returns (value ,index)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            label = labels[i]\n",
    "            pred = predicted[i]\n",
    "            if (label == pred):\n",
    "                n_class_correct[label] += 1\n",
    "            n_class_samples[label] += 1\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network: {acc} %')\n",
    "\n",
    "    for i in range(10):\n",
    "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "        print(f'Accuracy of {classes[i]}: {acc} %')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='transfer_learning' name='transfer_learning'><h1>Transfer Learning</h1></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.array([0.5, 0.5, 0.5])\n",
    "std = np.array([0.25, 0.25, 0.25])\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = 'data/hymenoptera_data'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                             shuffle=True, num_workers=0)\n",
    "              for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "imshow(out, title=[class_names[x] for x in classes])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Finetuning the convnet ####\n",
    "# Load a pretrained model and reset final fully connected layer.\n",
    "\n",
    "model = models.resnet18(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "# Here the size of each output sample is set to 2.\n",
    "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "model.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# StepLR Decays the learning rate of each parameter group by gamma every step_size epochs\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "# Learning rate scheduling should be applied after optimizer’s update\n",
    "# e.g., you should write your code this way:\n",
    "# for epoch in range(100):\n",
    "#     train(...)\n",
    "#     validate(...)\n",
    "#     scheduler.step()\n",
    "\n",
    "step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "model = train_model(model, criterion, optimizer, step_lr_scheduler, num_epochs=25)\n",
    "\n",
    "\n",
    "#### ConvNet as fixed feature extractor ####\n",
    "# Here, we need to freeze all the network except the final layer.\n",
    "# We need to set requires_grad == False to freeze the parameters so that the gradients are not computed in backward()\n",
    "model_conv = torchvision.models.resnet18(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized as\n",
    "# opposed to before.\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n",
    "\n",
    "model_conv = train_model(model_conv, criterion, optimizer_conv,\n",
    "                         exp_lr_scheduler, num_epochs=25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='tensorboard' name='tensorboard'><h1>Tensorboard</h1></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "############## TENSORBOARD ########################\n",
    "import sys\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('runs/mnist')\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters \n",
    "input_size = 784 # 28x28\n",
    "hidden_size = 500 \n",
    "num_classes = 10\n",
    "num_epochs = 1\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "\n",
    "# MNIST dataset \n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),  \n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "examples = iter(test_loader)\n",
    "example_data, example_targets = examples.next()\n",
    "\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.imshow(example_data[i][0], cmap='gray')\n",
    "#plt.show()\n",
    "\n",
    "############## TENSORBOARD ########################\n",
    "img_grid = torchvision.utils.make_grid(example_data)\n",
    "writer.add_image('mnist_images', img_grid)\n",
    "writer.flush()\n",
    "#sys.exit()\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected neural network with one hidden layer\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.l1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        # no activation and no softmax at the end\n",
    "        return out\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "############## TENSORBOARD ########################\n",
    "writer.add_graph(model, example_data.reshape(-1, 28*28))\n",
    "writer.flush()\n",
    "#writer.close()\n",
    "#sys.exit()\n",
    "###################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [100/938], Loss: 0.3010\n",
      "Epoch [1/1], Step [200/938], Loss: 0.4586\n",
      "Epoch [1/1], Step [300/938], Loss: 0.2836\n",
      "Epoch [1/1], Step [400/938], Loss: 0.1568\n",
      "Epoch [1/1], Step [500/938], Loss: 0.2066\n",
      "Epoch [1/1], Step [600/938], Loss: 0.4916\n",
      "Epoch [1/1], Step [700/938], Loss: 0.1423\n",
      "Epoch [1/1], Step [800/938], Loss: 0.2659\n",
      "Epoch [1/1], Step [900/938], Loss: 0.1538\n",
      "Accuracy of the network on the 10000 test images: 96.46 %\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "running_loss = 0.0\n",
    "running_correct = 0\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # origin shape: [100, 1, 28, 28]\n",
    "        # resized: [100, 784]\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        running_correct += (predicted == labels).sum().item()\n",
    "        if (i+1) % 100 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "            ############## TENSORBOARD ########################\n",
    "            writer.add_scalar('training loss', running_loss / 100, epoch * n_total_steps + i)\n",
    "            running_accuracy = running_correct / 100 / predicted.size(0)\n",
    "            writer.add_scalar('accuracy', running_accuracy, epoch * n_total_steps + i)\n",
    "            running_correct = 0\n",
    "            running_loss = 0.0\n",
    "            ###################################################\n",
    "\n",
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "class_labels = []\n",
    "class_preds = []\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        # max returns (value ,index)\n",
    "        values, predicted = torch.max(outputs.data, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        class_probs_batch = [F.softmax(output, dim=0) for output in outputs]\n",
    "\n",
    "        class_preds.append(class_probs_batch)\n",
    "        class_labels.append(predicted)\n",
    "\n",
    "    # 10000, 10, and 10000, 1\n",
    "    # stack concatenates tensors along a new dimension\n",
    "    # cat concatenates tensors in the given dimension\n",
    "    class_preds = torch.cat([torch.stack(batch) for batch in class_preds])\n",
    "    class_labels = torch.cat(class_labels)\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network on the 10000 test images: {acc} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ############## TENSORBOARD ########################\n",
    "    classes = range(10)\n",
    "    for i in classes:\n",
    "        labels_i = class_labels == i\n",
    "        preds_i = class_preds[:, i]\n",
    "        writer.add_pr_curve(str(i), labels_i, preds_i, global_step=0)\n",
    "        writer.close()\n",
    "    ###################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='model_save_and_load' name='model_save_and_load'><h1>Model save and load</h1></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 3 DIFFERENT METHODS TO REMEMBER:\n",
    " - torch.save(arg, PATH) # can be model, tensor, or dictionary\n",
    " - torch.load(PATH)\n",
    " - torch.load_state_dict(arg)\n",
    "'''\n",
    "\n",
    "''' 2 DIFFERENT WAYS OF SAVING\n",
    "# 1) lazy way: save whole model\n",
    "torch.save(model, PATH)\n",
    "\n",
    "# model class must be defined somewhere\n",
    "model = torch.load(PATH)\n",
    "model.eval()\n",
    "\n",
    "# 2) recommended way: save only the state_dict\n",
    "torch.save(model.state_dict(), PATH)\n",
    "\n",
    "# model must be created again with parameters\n",
    "model = Model(*args, **kwargs)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()\n",
    "'''\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, n_input_features):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = nn.Linear(n_input_features, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_pred = torch.sigmoid(self.linear(x))\n",
    "        return y_pred\n",
    "\n",
    "model = Model(n_input_features=6)\n",
    "# train your model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################save all ######################################\n",
    "for param in model.parameters():\n",
    "    print(param)\n",
    "\n",
    "# save and load entire model\n",
    "\n",
    "FILE = \"model.pth\"\n",
    "torch.save(model, FILE)\n",
    "\n",
    "loaded_model = torch.load(FILE)\n",
    "loaded_model.eval()\n",
    "\n",
    "for param in loaded_model.parameters():\n",
    "    print(param)\n",
    "\n",
    "\n",
    "############save only state dict #########################\n",
    "\n",
    "# save only state dict\n",
    "FILE = \"model.pth\"\n",
    "torch.save(model.state_dict(), FILE)\n",
    "\n",
    "print(model.state_dict())\n",
    "loaded_model = Model(n_input_features=6)\n",
    "loaded_model.load_state_dict(torch.load(FILE)) # it takes the loaded dictionary, not the path file itself\n",
    "loaded_model.eval()\n",
    "\n",
    "print(loaded_model.state_dict())\n",
    "\n",
    "\n",
    "###########load checkpoint#####################\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "checkpoint = {\n",
    "\"epoch\": 90,\n",
    "\"model_state\": model.state_dict(),\n",
    "\"optim_state\": optimizer.state_dict()\n",
    "}\n",
    "print(optimizer.state_dict())\n",
    "FILE = \"checkpoint.pth\"\n",
    "torch.save(checkpoint, FILE)\n",
    "\n",
    "model = Model(n_input_features=6)\n",
    "optimizer = optimizer = torch.optim.SGD(model.parameters(), lr=0)\n",
    "\n",
    "checkpoint = torch.load(FILE)\n",
    "model.load_state_dict(checkpoint['model_state'])\n",
    "optimizer.load_state_dict(checkpoint['optim_state'])\n",
    "epoch = checkpoint['epoch']\n",
    "\n",
    "model.eval()\n",
    "# - or -\n",
    "# model.train()\n",
    "\n",
    "print(optimizer.state_dict())\n",
    "\n",
    "# Remember that you must call model.eval() to set dropout and batch normalization layers \n",
    "# to evaluation mode before running inference. Failing to do this will yield \n",
    "# inconsistent inference results. If you wish to resuming training, \n",
    "# call model.train() to ensure these layers are in training mode.\n",
    "\n",
    "\"\"\" SAVING ON GPU/CPU \n",
    "\n",
    "# 1) Save on GPU, Load on CPU\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "torch.save(model.state_dict(), PATH)\n",
    "\n",
    "device = torch.device('cpu')\n",
    "model = Model(*args, **kwargs)\n",
    "model.load_state_dict(torch.load(PATH, map_location=device))\n",
    "\n",
    "# 2) Save on GPU, Load on GPU\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "torch.save(model.state_dict(), PATH)\n",
    "\n",
    "model = Model(*args, **kwargs)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.to(device)\n",
    "\n",
    "# Note: Be sure to use the .to(torch.device('cuda')) function \n",
    "# on all model inputs, too!\n",
    "\n",
    "# 3) Save on CPU, Load on GPU\n",
    "torch.save(model.state_dict(), PATH)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model = Model(*args, **kwargs)\n",
    "model.load_state_dict(torch.load(PATH, map_location=\"cuda:0\"))  # Choose whatever GPU device number you want\n",
    "model.to(device)\n",
    "\n",
    "# This loads the model to a given GPU device. \n",
    "# Next, be sure to call model.to(torch.device('cuda')) to convert the model’s parameter tensors to CUDA tensors\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='save_checkpoints' name='save_checkpoints'><h1>Save & Load checkpoints</h1></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.optim as optim  # For all Optimization algorithms, SGD, Adam, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Loading checkpoint\n"
     ]
    }
   ],
   "source": [
    "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint, model, optimizer):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    \n",
    "# Initialize network\n",
    "model = torchvision.models.vgg16(pretrained=False)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n",
    "# Try save checkpoint\n",
    "save_checkpoint(checkpoint)\n",
    "\n",
    "# Try load checkpoint\n",
    "load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='links' name='links'><h1>Links</h1>\n",
    "<ul><li><a href='https://github.com/python-engineer/pytorchTutorial'>https://github.com/python-engineer/pytorchTutorial</a>\n",
    "<li>https://github.com/jeffheaton/present/blob/master/youtube/pytorch/iris_pytorch.ipynb\n",
    "<li>https://datascience.stackexchange.com/questions/12321/whats-the-difference-between-fit-and-fit-transform-in-scikit-learn-models\n",
    "</ul></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
